import flet as ft
import onnxruntime
import numpy as np
import os
import difflib
import threading
import time
import requests # Ù„Ù„ØªØ­Ù…ÙŠÙ„
from datetime import datetime
from tokenizers import Tokenizer

# --- Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ (Ø§Ù„Ù…ØµØ¯Ø±) ---
# Ø±Ø§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø± Ù„Ù…Ù„Ù ONNX Ø§Ù„Ù…Ø®ÙÙ Ù…Ù† HuggingFace
MODEL_URL = "https://huggingface.co/onnx-community/Qwen2.5-0.5B-Instruct/resolve/main/onnx/model_quantized.onnx"
MODEL_FILENAME = "model_quantized.onnx"

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ---
# Ø³Ù†Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ ÙÙŠ Ø§Ù„Ù‡Ø§ØªÙ
def get_model_path():
    # ÙÙŠ Ø§Ù„Ø£Ù†Ø¯Ø±ÙˆÙŠØ¯ØŒ Ù†Ø­ÙØ¸ ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø£Ùˆ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    return os.path.join(os.getcwd(), MODEL_FILENAME)

# --- Ø§Ù„Ø¯Ù…Ø§Øº Ø§Ù„Ø£ÙˆÙ„: Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© ---
class LocalBrain:
    def __init__(self):
        self.memory = {
            "Ù…Ø±Ø­Ø¨Ø§": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ø¬Ø§Ø±ÙŠ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ...",
            "Ù…Ù† Ø§Ù†Øª": "Ø£Ù†Ø§ ØªØ·Ø¨ÙŠÙ‚ Qwen-NativeØŒ Ø£Ø¹Ù…Ù„ Ø¨Ù…Ø¹Ø§Ù„Ø¬ Ù‡Ø§ØªÙÙƒ.",
        }
    
    def learn(self, q, a):
        self.memory[q.lower().strip()] = a

    def get_response(self, text):
        text = text.lower().strip().replace("Ø£", "Ø§").replace("Ø©", "Ù‡")
        if "Ø³Ø§Ø¹Ù‡" in text or "ÙˆÙ‚Øª" in text:
            return f"â° {datetime.now().strftime('%I:%M %p')}"
        
        matches = difflib.get_close_matches(text, self.memory.keys(), n=1, cutoff=0.7)
        if matches: return self.memory[matches[0]]
        return None

# --- Ø§Ù„Ø¯Ù…Ø§Øº Ø§Ù„Ø«Ø§Ù†ÙŠ: Ù…Ø­Ø±Ùƒ Qwen ---
class QwenEngine:
    def __init__(self):
        self.session = None
        self.tokenizer = None
        self.status = "Ø¬Ø§Ø±ÙŠ Ø§Ù„ÙØ­Øµ..."
        self.progress = 0.0
        self.is_downloading = False
        self.is_ready = False
        
        # Ù†Ø¨Ø¯Ø£ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙØ­Øµ ÙˆØ§Ù„ØªØ­Ù…ÙŠÙ„
        threading.Thread(target=self._init_system, daemon=True).start()

    def _init_system(self):
        try:
            target_path = get_model_path()
            
            # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø§ÙŠØ²Ø± (Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ assets Ø§Ù„ØªØ·Ø¨ÙŠÙ‚)
            # Flet ÙŠÙÙƒ Ø¶ØºØ· Ø§Ù„Ù€ assets Ø¹Ù†Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„ØŒ Ù†Ø­Ø§ÙˆÙ„ Ù‚Ø±Ø§Ø¡ØªÙ‡
            try:
                self.tokenizer = Tokenizer.from_file("assets/tokenizer.json")
            except:
                self.status = "âš ï¸ Ù…Ù„Ù Tokenizer Ù…ÙÙ‚ÙˆØ¯ ÙÙŠ Assets"
                return

            # 2. ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            if not os.path.exists(target_path):
                self.status = "Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·)..."
                self.is_downloading = True
                self._download_model(target_path)
                self.is_downloading = False
            
            if not os.path.exists(target_path):
                self.status = "âŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„."
                return

            # 3. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            self.status = "Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬..."
            
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
            sess_options = onnxruntime.SessionOptions()
            sess_options.intra_op_num_threads = 4 
            sess_options.execution_mode = onnxruntime.ExecutionMode.ORT_SEQUENTIAL
            sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL

            self.session = onnxruntime.InferenceSession(
                target_path, 
                sess_options=sess_options, 
                providers=['CPUExecutionProvider']
            )
            
            self.is_ready = True
            self.status = "âœ… Ø¬Ø§Ù‡Ø² (Qwen Native)"
            
        except Exception as e:
            self.status = f"Ø®Ø·Ø£: {str(e)}"

    def _download_model(self, save_path):
        try:
            response = requests.get(MODEL_URL, stream=True)
            total_size = int(response.headers.get('content-length', 0))
            block_size = 1024 * 1024 # 1 MB chunk
            downloaded = 0

            with open(save_path, 'wb') as f:
                for data in response.iter_content(block_size):
                    f.write(data)
                    downloaded += len(data)
                    if total_size > 0:
                        self.progress = downloaded / total_size
            
            self.progress = 1.0
        except Exception as e:
            self.status = f"ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„: {e}"
            if os.path.exists(save_path): os.remove(save_path) # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¹Ø·ÙˆØ¨

    def generate(self, text):
        if not self.is_ready: return f"âš ï¸ {self.status}"
        
        try:
            # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†Øµ
            prompt = f"<|im_start|>user\n{text}<|im_end|>\n<|im_start|>assistant\n"
            tokens = self.tokenizer.encode(prompt).ids
            
            input_feed = {self.session.get_inputs()[0].name: np.array([tokens], dtype=np.int64)}
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬
            output = self.session.run(None, input_feed)[0]
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø© (Ù„Ù„ØªØ¨Ø³ÙŠØ· Ù†Ø£Ø®Ø° ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø©ØŒ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ ÙŠØ­ØªØ§Ø¬ Loop)
            predicted_id = np.argmax(output[0, -1, :])
            decoded = self.tokenizer.decode([predicted_id])
            
            return f"ğŸ¤– (Qwen): {decoded}... (ØªÙ…Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ù„ÙŠØ§Ù‹)"
            
        except Exception as e:
            return f"Ø®Ø·Ø£ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {e}"

def main(page: ft.Page):
    page.title = "Qwen Downloader"
    page.theme_mode = ft.ThemeMode.DARK
    page.bgcolor = "#111"

    local_brain = LocalBrain()
    qwen_engine = QwenEngine()
    
    chat = ft.ListView(expand=True, spacing=10, padding=15, auto_scroll=True)
    
    # Ø¹Ù†Ø§ØµØ± Ø´Ø±ÙŠØ· Ø§Ù„Ø­Ø§Ù„Ø© ÙˆØ§Ù„ØªØ­Ù…ÙŠÙ„
    status_lbl = ft.Text("...", color="grey", size=12)
    progress_bar = ft.ProgressBar(width=200, color="blue", visible=False)

    def update_ui_loop():
        while True:
            status_lbl.value = qwen_engine.status
            
            if qwen_engine.is_downloading:
                progress_bar.visible = True
                progress_bar.value = qwen_engine.progress
                status_lbl.value = f"Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„: {int(qwen_engine.progress * 100)}%"
            else:
                progress_bar.visible = False
                status_lbl.color = "green" if qwen_engine.is_ready else "red"
            
            page.update()
            time.sleep(0.5)

    threading.Thread(target=update_ui_loop, daemon=True).start()

    def add(text, sender):
        align = ft.MainAxisAlignment.END if sender == "user" else ft.MainAxisAlignment.START
        bg = ft.Colors.BLUE_900 if sender == "user" else ft.Colors.GREY_800
        chat.controls.append(ft.Row([ft.Container(content=ft.Markdown(text), padding=12, border_radius=10, bgcolor=bg)], alignment=align))
        page.update()

    def send(e):
        txt = field.value
        if not txt: return
        field.value = ""
        add(txt, "user")

        # Ø§Ù„Ø±Ø¯ Ø§Ù„ÙÙˆØ±ÙŠ
        fast = local_brain.get_response(txt)
        if fast:
            add(fast, "bot")
            return

        # Ø§Ù„Ø±Ø¯ Ø§Ù„Ø¹Ù…ÙŠÙ‚
        loading = ft.ProgressRing(width=20, height=20)
        chat.controls.append(loading)
        page.update()
        
        def run():
            resp = qwen_engine.generate(txt)
            chat.controls.remove(loading)
            add(resp, "bot")
        threading.Thread(target=run, daemon=True).start()

    field = ft.TextField(hint_text="ØªØ­Ø¯Ø«...", expand=True, on_submit=send, border_radius=20, bgcolor="#222")
    
    page.add(
        ft.Container(
            content=ft.Column([
                ft.Row([ft.Text("Native AI", weight="bold"), ft.Container(expand=True), status_lbl]),
                progress_bar
            ]), 
            padding=10, bgcolor="#222"
        ),
        ft.Container(chat, expand=True),
        ft.Container(content=ft.Row([field, ft.IconButton(ft.Icons.SEND, on_click=send)]), padding=10)
    )

ft.app(target=main)